{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statsmodels\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "group_cols =['Category', 'Segment', 'Sub_Category', 'Account','Date']\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "import os\n",
    "import math\n",
    "# from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "# ---------------------------------------- INPUT ---------------------------------------------------#\n",
    "#   Directory to read files from.... ex: C:/files/Input files                                       #\n",
    "    files_to_read_from='C:/Users/MalikM/Documents/VRS -  Roaming Services/MOC_INBOUND/Usable'\n",
    "# --------------------------------------------------------------------------------------------------# \n",
    "\n",
    "\n",
    "# ---------------------------------------- INPUT ---------------------------------------------------#    \n",
    "#   Directory to write output files to... ex: C:/files/Output files/\n",
    "    output_path='C:/Users/MalikM/Documents/VRS -  Roaming Services/MOC_INBOUND/OUTPUT_MOC_INBOUND/with 3M testing/'\n",
    "# --------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    all_files_list=os.listdir(files_to_read_from)\n",
    "    summary_file=pd.DataFrame()\n",
    "    summary_less_than_40=pd.DataFrame()\n",
    "    all_pred=pd.DataFrame()\n",
    "    for i in range(len(all_files_list)):\n",
    "        print(\"*******\",i,'*******')\n",
    "        filename=''\n",
    "        filename=all_files_list[i][:-4] ###Removing .csv from filename###\n",
    "        print('filename---',filename)\n",
    "        output_filename='output_'+filename+'.csv'\n",
    "        print('output filename---',output_filename)\n",
    "        data_=pd.DataFrame()\n",
    "        df=files_to_read_from+'/'+all_files_list[i]\n",
    "        print('df=',df)\n",
    "        data_=pd.read_csv(df)           ###Reading individual files in loop###\n",
    "        print('data read')\n",
    "        data=pd.DataFrame(data_) \n",
    "\n",
    "# ---------------------------------------- INPUT ---------------------------------------------------#         \n",
    "# Enter the column name you want to apply time series on                                            #\n",
    "        COLUMN_NAME='INBOUND_DURATION'#'INBOUND_VOL_MB'#'OUTBOUND_VOL_MB'#'OUTBOUND_DURATION' \n",
    "# Date column for sorting the data                                                                  #\n",
    "        data.sort_values(by=['CALL_YEAR_MONTH'], axis=0, inplace=True)\n",
    "# --------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "        \n",
    "        data.reset_index(inplace = True, drop = True)\n",
    "        print('length of unique values',len(np.unique(data[COLUMN_NAME])))\n",
    "        print('length of file',len(data))\n",
    "        \n",
    "        \n",
    "# ---------------------------------------- INPUT ---------------------------------------------------# \n",
    "# Enter length of train data you want to train model with                                           #                                                                   #\n",
    "        sample_data_length=36         \n",
    "# Enter length of test data you want to test model on                                               #\n",
    "        test_data_length=3\n",
    "#---------------------------------------------------------------------------------------------------# \n",
    "\n",
    "\n",
    "        total=sample_data_length+test_data_length\n",
    "        print('minimum length required', total)\n",
    "        if len(data)>total and len(np.unique(data[COLUMN_NAME]))>8 :\n",
    "            sample_data=data[len(data)-total:len(data)-test_data_length] \n",
    "            test= data[len(data)-test_data_length: len(data)]\n",
    "            print('sample_data and test data read')\n",
    "            best_forecasted_ets=pd.DataFrame()\n",
    "            ets_comparison=pd.DataFrame()\n",
    "            \n",
    "# ---------------------------------------- INPUT --------------------------------------------------------# \n",
    "# 4 months test->19,   6 months test->21,  12 months test->27 ....(test data size + n forecast, here 15) #\n",
    "            test_plus_Nforecast=15\n",
    "            a =main(sample_data,test,test_plus_Nforecast,output_path,output_filename,COLUMN_NAME) \n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "            a['file_name']=filename\n",
    "            print('final output--',a)\n",
    "        \n",
    "\n",
    "# ---------------------------------------- INPUT --------------------------------------------------------# \n",
    "# Summary file takes the first predicted value .......(4 m test- [4:5], 6m test- [6:7], 12m test-[12:13])#\n",
    "            summary_file=summary_file.append(a[3:4]) \n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "            all_pred=all_pred.append(a)\n",
    "         \n",
    "        else:\n",
    "            print(' *******', filename, '- has less records for training and testing or same values********* ')\n",
    "            path_filename=save(output_path,output_filename)\n",
    "            summary_less_than_40=summary_less_than_40.append({'filename':filename},ignore_index=True)\n",
    "\n",
    "    return all_pred,summary_file , summary_less_than_40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sample_data,test,test_plus_Nforecast,output_path,output_filename,COLUMN_NAME):\n",
    "    best_forecasted_ets=pd.DataFrame()\n",
    "    ets_comparison=pd.DataFrame()\n",
    "    forecasted_arima=pd.DataFrame()\n",
    "    print('forecasted Ets below')\n",
    "    \n",
    "# *** Calling get_best_forecasts which internally calls - simple exp smooting, holts and holts-winter*** #\n",
    "    best_forecasted_ets, ets_comparison =get_best_forecasts(sample_data,test,test_plus_Nforecast,COLUMN_NAME)\n",
    "    \n",
    "    print('ets_comparison--', ets_comparison)\n",
    "    print('best_forecasted_ets--',best_forecasted_ets)\n",
    "\n",
    "# *** Calling _compute_arima_per_grain                                                               *** #\n",
    "    forecasted_arima=_compute_arima_per_grain(sample_data,test,test_plus_Nforecast,COLUMN_NAME)\n",
    "    \n",
    "    forecasted_arima.reset_index(inplace = True, drop = True)\n",
    "    forecasted_arima.Mape=forecasted_arima.Mape.astype(float)\n",
    "    forecasted_arima=forecasted_arima.sort_values(by=['Mape'])\n",
    "    print('forecasted arima is--', forecasted_arima[0:28])\n",
    "    forecasted_arima_mape_=pd.DataFrame()\n",
    "    forecasted_arima_mape_=forecasted_arima_mape_.append(forecasted_arima)\n",
    "    forecasted_arima_mape=forecasted_arima_mape_[0:test_plus_Nforecast].sort_index(axis = 0,ignore_index=True)\n",
    "    forecasted_arima_mape.reset_index(inplace=True, drop=True)\n",
    "    param=''\n",
    "    Mape=''\n",
    "    param=forecasted_arima_mape['param_val'][0]\n",
    "    print('Arima param is--', param)\n",
    "    Mape=forecasted_arima_mape['Mape'][0]\n",
    "    print('Arima Mape is--',Mape)\n",
    "    param=tuple(map(int, param.replace('(','').replace(')','').split(',') ))\n",
    "    best_forecasted_arima=pd.DataFrame()\n",
    "#     print('forecasted_arima_mape--', forecasted_arima_mape[0:28])\n",
    "# *** Calling arima which calculates forecasted values on rolling one month arima output             *** #\n",
    "    best_forecasted_arima=arima(sample_data,param,test_plus_Nforecast,Mape,COLUMN_NAME)\n",
    "#     print('rolling rima outpot is-----', best_forecasted_arima)\n",
    "    final_df=pd.DataFrame()\n",
    "    \n",
    "    final_df=final_output(forecasted_arima_mape,best_forecasted_ets,best_forecasted_arima,test_plus_Nforecast)\n",
    "    path_filename=''\n",
    "    path_filename=save(output_path,output_filename)\n",
    "    final_df.to_csv(path_filename)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa,bb,cc=read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa-> All prediction file\n",
    "#bb-> summary file, which appends first prediction with mape value for all series \n",
    "#cc-> the unusable series\n",
    "aa.to_csv('C:/Users/MalikM/Documents/VRS -  Roaming Services/MOC_INBOUND/OUTPUT_MOC_INBOUND/with 3M testing/MOC_INBOUND_IN_all_pred_03mtest_13Apr.csv')\n",
    "bb.to_csv('C:/Users/MalikM/Documents/VRS -  Roaming Services/MOC_INBOUND/OUTPUT_MOC_INBOUND/with 3M testing/MOC_INBOUND_IN_summary_file_03mtest_13Apr.csv')\n",
    "cc.to_csv('C:/Users/MalikM/Documents/VRS - Roaming Services/MOC_INBOUND/OUTPUT_MOC_INBOUND/with 3M testing/MOC_INBOUND_IN_unpredicted_series_03mtest_13Apr.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALLED FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### holt winter\n",
    "def grid_search_exp_sm(ts1, test,COLUMN_NAME): ## no direct method for mape calculation with exponential smooting, so taking 3 for test and calculate mape with this\n",
    "    print('holt winter started..')\n",
    "    forecast_list = []\n",
    "    seasonality = np.arange(2,13,1)\n",
    "    seasonal =['add']\n",
    "    trend = ['add']\n",
    "    for i in seasonal:\n",
    "        for j in seasonality:\n",
    "            for k in trend:\n",
    "                fit = ExponentialSmoothing(ts1[COLUMN_NAME], seasonal_periods=j, trend=k, seasonal=i).fit()\n",
    "                forecasts=fit.forecast(len(test))\n",
    "#                 mape=100* np.mean(abs(np.sum(test['OUTBOUND_VOL_MB'])- np.sum(forecasts[1:5]))/np.sum(test['OUTBOUND_VOL_MB']))\n",
    "                \n",
    "#                 forecasts=pd.concat([forecasts[1:5],test['OUTBOUND_VOL_MB']],axis=1)\n",
    "                y_true, y_pred = np.array(test[COLUMN_NAME]), np.array(forecasts)\n",
    "                mape=np.mean(np.abs((np.subtract(y_true , y_pred)) / y_true)*100) \n",
    "#                 mape=pd.to_numeric(mape, errors='coerce')        \n",
    "        \n",
    "                result = seasonal_decompose(forecasts, freq=3, model='additive')#should we decompose forecast or ts1 ---- TS1\n",
    "                error=result.resid\n",
    "#                 mape= 100*np.mean(np.abs((np.sum(error)/np.sum(forecasts))))\n",
    "                forecast_list.append({'Trend': k,'Seasonality': j, 'seasonal': i, 'Mape': mape,'method' :'Exponential'})\n",
    "    forecast_list = pd.DataFrame(forecast_list).sort_values(by =['Mape'])\n",
    "    return forecast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple exp smooting\n",
    "def simple_exp_sm(ts1,test,COLUMN_NAME):\n",
    "    print('simple exp smoothing started')\n",
    "    forecast_list = []\n",
    "    smoothing_level = np.arange(0.01,1.01,.01)\n",
    "    for i in smoothing_level:        \n",
    "        fit = SimpleExpSmoothing(ts1[COLUMN_NAME]).fit(smoothing_level=i,optimized=False)\n",
    "        forecasts = fit.forecast(len(test))\n",
    "#         error = mape(ts2,forecasts)\n",
    "        result = seasonal_decompose(forecasts, freq=3, model='additive')#should we decompose forecast or ts1 ---- TS1\n",
    "        error=result.resid\n",
    "        \n",
    "        y_true, y_pred = np.array(test[COLUMN_NAME]), np.array(forecasts)\n",
    "        mape=np.mean(np.abs((np.subtract(y_true , y_pred)) / y_true)*100) \n",
    "#         mape=pd.to_numeric(mape, errors='coerce')\n",
    "#         mape=100* np.mean(abs(np.sum(test['OUTBOUND_VOL_MB'])- np.sum(forecasts[1:5]))/np.sum(test['OUTBOUND_VOL_MB']))\n",
    "\n",
    "#         mape= 100*np.mean(np.abs(np.sum(error)/np.sum(forecasts))) ## decompo\n",
    "#         mape## decompose and take mean of absolute values of errors\n",
    "            \n",
    "        forecast_list.append({'smoothing_level': i, 'Mape':mape, 'method': 'Simple Exp'})\n",
    "    forecast_list = pd.DataFrame(forecast_list).sort_values(by =['Mape'])\n",
    "    return forecast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holts\n",
    "def holts(ts1,test,COLUMN_NAME): # triple smoothing\n",
    "    print('holts started..')\n",
    "    forecast_list = []\n",
    "    smoothing_level = np.arange(0.1,1.1,0.1)\n",
    "    smoothing_slope = np.arange(0.1,1.1,0.1)\n",
    "    for i in smoothing_level:  \n",
    "        for j in smoothing_slope:\n",
    "            fit = Holt(ts1[COLUMN_NAME]).fit(smoothing_level=i, smoothing_slope=j, optimized=False)\n",
    "            forecasts = fit.forecast(len(test))\n",
    "#             error = mape(ts2,forecast)\n",
    "            result = seasonal_decompose(forecasts, freq=3, model='additive')#should we decompose forecast or ts1 ---- TS1\n",
    "            error=result.resid\n",
    "            y_true, y_pred = np.array(test[COLUMN_NAME]), np.array(forecasts)\n",
    "            mape=np.mean(np.abs((np.subtract(y_true , y_pred)) / y_true)*100)\n",
    "#             mape=pd.to_numeric(mape, errors='coerce')\n",
    "#             mape=100* np.mean(np.abs(np.sum(test['OUTBOUND_VOL_MB'])- np.sum(forecasts[1:5]))/np.sum(test['OUTBOUND_VOL_MB']))\n",
    "#             mape=100*np.mean(np.sum(np.abs(test['OUTBOUND_VOL_MB']-forecasts[1:4]/test['OUTBOUND_VOL_MB'])))\n",
    "#             mape= 100*np.mean(np.abs((np.sum(error)/np.sum(forecasts)))) ## decompo\n",
    "\n",
    "            forecast_list.append({'smoothing_level': i,'smoothing_slope' : j,'Mape': mape, 'method' :  'Holt'})\n",
    "    forecast_list = pd.DataFrame(forecast_list).sort_values(by =['Mape'])\n",
    "    return forecast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_forecasts(ts1,test,horizon,COLUMN_NAME):\n",
    "    print('Smoothing started')\n",
    "    forecast = pd.DataFrame()\n",
    "    forecast_df1 = pd.DataFrame()\n",
    "    forecast_df2 = pd.DataFrame()\n",
    "    forecast_df3 = pd.DataFrame()\n",
    "    forecast_df1 = grid_search_exp_sm(ts1,test,COLUMN_NAME)\n",
    "    forecast_df2 = simple_exp_sm(ts1,test,COLUMN_NAME)\n",
    "    forecast_df3 = holts(ts1,test,COLUMN_NAME)\n",
    "    final_df = pd.DataFrame()\n",
    "    final_df = pd.concat([forecast_df1.head(1),forecast_df2.head(1),forecast_df3.head(1)])\n",
    "    final_df = final_df.sort_values(by =['Mape'],ascending=True,na_position = 'last')\n",
    "    final_df.fillna('NA', inplace = True)\n",
    "    final_df.reset_index(drop = True)\n",
    "    method, mape='',''\n",
    "    method = final_df['method'].head(1)\n",
    "    mape_=final_df['Mape'].head(1)\n",
    "    mape=mape_.item()\n",
    "    if method.item() =='Exponential':\n",
    "        print('Triple Exponential Method Chosen')\n",
    "        seasonality = forecast_df1['Seasonality'].head(1).item()\n",
    "        trend = forecast_df1['Trend'].head(1).item()\n",
    "        seasonal = forecast_df1['seasonal'].head(1).item()\n",
    "        fit = ExponentialSmoothing(ts1[COLUMN_NAME], seasonal_periods=seasonality, trend=trend, seasonal=seasonal).fit()\n",
    "#         fitted = fit.predict(ts1.index.min(),ts1.index.max())\n",
    "        forecast = (fit.forecast(horizon))\n",
    "    elif method.item() =='Simple Exp':\n",
    "        print('Simple Method Chosen')\n",
    "        smoothing_level = forecast_df2['smoothing_level'].head(1).item()\n",
    "        fit = SimpleExpSmoothing(ts1[COLUMN_NAME]).fit(smoothing_level=smoothing_level,optimized=False)\n",
    "#         fitted = fit.predict(ts1.index.min(),ts1.index.max())\n",
    "        forecast = (fit.forecast(horizon))\n",
    "#         return forecast\n",
    "    elif method.item() =='Holt':\n",
    "        print('Holt Method Chosen')\n",
    "        smoothing_level = forecast_df3['smoothing_level'].head(1).item()\n",
    "        smoothing_slope = forecast_df3['smoothing_slope'].head(1).item()\n",
    "        \n",
    "        fit = Holt(ts1[COLUMN_NAME]).fit(smoothing_level=smoothing_level, smoothing_slope=smoothing_slope, optimized=False)\n",
    "#         fitted = fit.predict(ts1.index.min(),ts1.index.max())\n",
    "        forecast = fit.forecast(horizon)\n",
    "    \n",
    "   \n",
    "    forecast = pd.DataFrame({ 'PointForecast':forecast.values})\n",
    "    \n",
    "    forecast['ModelName']= 'ETS'\n",
    "    forecast['Mape'] =mape\n",
    "#     fitted = pd.DataFrame({'Date':ts1.index, 'ETS':fitted.values}).set_index('Date')\n",
    "    return forecast,final_df#,fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _compute_arima_per_grain(ts1,test,horizon,COLUMN_NAME):\n",
    "    print('arima started...')\n",
    "    forecast_concat = pd.DataFrame()\n",
    "    arima_forecast= pd.DataFrame()\n",
    "    arima_ordera_errors=pd.DataFrame()\n",
    "    yhat=pd.DataFrame()\n",
    "\n",
    "\n",
    "    p  = range(0, 3)\n",
    "    d = q = range(0,3)\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "        \n",
    "    history = [x for x in ts1[COLUMN_NAME]]\n",
    "\n",
    "    for x in pdq:\n",
    "        arima_order = list(x)\n",
    "\n",
    "        try:\n",
    "            arima_output=pd.DataFrame()\n",
    "            arima_model = ARIMA(history, order=arima_order)\n",
    "            arima_fit = arima_model.fit(disp=0)\n",
    "            resid=pd.DataFrame(arima_fit.resid)\n",
    "            fitted=arima_fit.fittedvalues\n",
    "           \n",
    "\n",
    "            arima_output = arima_fit.forecast(horizon)\n",
    "            y_true, y_pred = np.array(test[COLUMN_NAME]), np.array(arima_output[0][0:len(test)])\n",
    "            mape=100*(np.mean(np.abs((y_true - y_pred) / y_true) ))\n",
    "\n",
    "#             mape=100* np.mean(abs(np.sum(test['OUTBOUND_VOL_MB'])- np.sum(arima_output[0][1:5]))/np.sum(test['OUTBOUND_VOL_MB']))\n",
    "            yhat = pd.DataFrame(arima_output[0],columns=(['PointForecast']))\n",
    "#             \n",
    "            yhat['ModelName']='Arima'\n",
    "            yhat['param_val']='{}'.format(tuple(arima_order))\n",
    "            \n",
    "            yhat['Mape']='{}'.format(mape)\n",
    "#             \n",
    "            arima_forecast=arima_forecast.append(yhat)#.sort_values(by=['Mape'])\n",
    "            \n",
    "        except:\n",
    "            print(arima_order, 'error...', end=\"\")\n",
    "            continue\n",
    "    return arima_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need to run this\n",
    "# forecasted_arima=_compute_arima_per_grain(sample_data,test,15)\n",
    "\n",
    "# forecasted_arima.reset_index(inplace = True, drop = True)\n",
    "# # forecasted_arima\n",
    "# forecasted_arima=forecasted_arima.sort_values(by=['Mape'])\n",
    "# forecasted_arima.Mape=forecasted_arima.Mape.astype(float)\n",
    "# forecasted_arima_mape_=pd.DataFrame()\n",
    "# forecasted_arima_mape_=forecasted_arima_mape_.append(forecasted_arima)\n",
    "# forecasted_arima_mape=forecasted_arima_mape_[0:15].sort_index(axis = 0,ignore_index=True)\n",
    "# #taking out pdq of the least mape from compute_grain_arima\n",
    "# param=forecasted_arima_mape['param_val'][0]\n",
    "\n",
    "# param=tuple(map(int, param.replace('(','').replace(')','').split(',') ))\n",
    "# Mape= forecasted_arima_mape['Mape']\n",
    "# param\n",
    "# Mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rolling arima forecast, with pdq values taken from least mape\n",
    "\n",
    "def arima(sample_data,param,horizon,Mape,COLUMN_NAME):\n",
    "    print('rolling arima started...')\n",
    "\n",
    "    history=list()\n",
    "    history = [x for x in sample_data[COLUMN_NAME]]\n",
    "\n",
    "    best_forecasted_arima = list()\n",
    "    for t in range(horizon):\n",
    "\n",
    "        try:\n",
    "            model = ARIMA(history, order=param)\n",
    "            model_fit = model.fit(disp=0)\n",
    "            output=list()\n",
    "            output = model_fit.forecast()\n",
    "            yhat = output[0][0]\n",
    "            best_forecasted_arima.append(yhat)\n",
    "#         print(pd.DataFrame(list(predictions)))\n",
    "            history.append(yhat)\n",
    "\n",
    "            a = pd.DataFrame(best_forecasted_arima,columns=(['PointForecast']))\n",
    "\n",
    "            a['ModelName']='Arima'\n",
    "            a['param_val']='{}'.format(param)\n",
    "            a['Mape']='{}'.format(Mape)\n",
    "        except:\n",
    "            pass        \n",
    "\n",
    "    return pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_output(arima_df_mape,ets_df_mape,best_forecasted_arima,test_plus_Nforecast): # (arima df with mape, ets df with mape,rollingarima forecast)\n",
    "    print('final output started...')\n",
    "    mape_arima=arima_df_mape['Mape'].head(1)\n",
    "    mape_ets=ets_df_mape['Mape'].head(1)\n",
    "\n",
    "    final_forecasted_df=pd.DataFrame()\n",
    "#     \n",
    "#     if len(best_forecasted_arima)==1:\n",
    "#         final_forecasted_df=final_forecasted_df.append(ets_df_mape)\n",
    "    if math.isnan(mape_arima.item()  ):\n",
    "        final_forecasted_df=final_forecasted_df.append(ets_df_mape)\n",
    "    elif mape_arima.item()< mape_ets.item() and len(best_forecasted_arima)==test_plus_Nforecast:\n",
    "        final_forecasted_df=final_forecasted_df.append(best_forecasted_arima)\n",
    "    elif mape_arima.item()< mape_ets.item() and len(best_forecasted_arima)< test_plus_Nforecast:\n",
    "        final_forecasted_df=final_forecasted_df.append(arima_df_mape)\n",
    "    elif mape_arima.item()> mape_ets.item():\n",
    "        final_forecasted_df=final_forecasted_df.append(ets_df_mape)\n",
    "    return final_forecasted_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(path,filename):\n",
    "    print('saving process started...')\n",
    "    output_file='OUTPUT_'+filename\n",
    "    path_output_file=path+output_file\n",
    "    return path_output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## consolidating all raw data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outbound- SRC_TADIG - DST_COUNTRY - DST_TADIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data():\n",
    "    # GPRS Outbound\n",
    "#     files_to_read_from='C:/Users/MalikM/Documents/VRS -  Roaming Services/GPRS_OUTBOUND/testing 10 series/All_usable_gprs_outbound/Usable_GPRS'\n",
    "\n",
    "    #GPRS Inbound\n",
    "#     files_to_read_from='C:/Users/MalikM/Documents/VRS - Roaming Services/GPRS_INBOUND/Usable'\n",
    "    \n",
    "    #MOC Inbound\n",
    "#     files_to_read_from='C:/Users/MalikM/Documents/VRS -  Roaming Services/MOC_INBOUND/Usable'\n",
    "    \n",
    "#     MOC Outbound\n",
    "    files_to_read_from='C:/Users/MalikM/Documents/VRS -  Roaming Services/MOC_OUTBOUND/Usable'\n",
    "    \n",
    "    all_files_list=os.listdir(files_to_read_from)\n",
    "    all_data=pd.DataFrame()\n",
    "    for i in range(len(all_files_list)):\n",
    "        print(\"*******\",i,'*******')\n",
    "        filename=''\n",
    "        df=''\n",
    "        filename=all_files_list[i][:-4] #########\n",
    "        print('filename---',filename)\n",
    "        data_=pd.DataFrame()\n",
    "        df=files_to_read_from+'/'+all_files_list[i]\n",
    "        print('df=',df)\n",
    "        data_=pd.read_csv(df)\n",
    "        print('data read')\n",
    "        data=pd.DataFrame(data_)\n",
    "        \n",
    "        #OUTBOUND\n",
    "        all_data=all_data.append(data[['CALL_YEAR_MONTH','SRC_TADIG','DST_TADIG','CALL_TYPE','SRC_COUNTRY','DST_COUNTRY']].head(1))\n",
    "        \n",
    "        #INBOUND\n",
    "#         all_data=all_data.append(data[['CALL_YEAR_MONTH','SRC_TADIG','DST_TADIG','CALL_TYPE','SRC_COUNTRY','DST_COUNTRY','DST_NETWORK']].head(1))\n",
    "        \n",
    "        END_DATE= data['CALL_YEAR_MONTH'].iloc[-1]\n",
    "        print(END_DATE)\n",
    "        all_data['END_DATE']=END_DATE\n",
    "#         all_data=all_data.append(data.tail(1))\n",
    "        print('length =',len(all_data))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=all_data()\n",
    "# outbound- SRC_TADIG -  DST_COUNTRY - DST_TADIG\n",
    "#inbound - \"SRC_TADIG\" - DST_COUNTRY\" - DST_TADIG\" - \"DST_NETWORK\"- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SRC_TADIG'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ervices/MOC_OUTBOUND/MOC_OUTBOUND_OUTPUT_36_12/MOC_OB_1row_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_predicted():\n",
    "    # GPRS Outbound\n",
    "#     files_to_read_from='C:/Users/MalikM/Documents/VRS - Roaming Services/GPRS_OUTBOUND/testing 10 series/All_usable_gprs_outbound/Usable_GPRS'\n",
    "\n",
    "    #GPRS Inbound\n",
    "#     files_to_read_from='C:/Users/MalikM/Documents/VRS -  Roaming Services/GPRS_INBOUND/Usable'\n",
    "    \n",
    "    #MOC Inbound\n",
    "#     files_to_read_from='C:/Users/MalikM/Documents/VRS -  Roaming Services/MOC_INBOUND/Usable'\n",
    "    \n",
    "#     MOC Outbound\n",
    "    files_to_read_from='C:/Users/MalikM/Documents/VRS - Roaming Services/MOC_OUTBOUND/MOC_OUTBOUND_OUTPUT_36_12/MOC_OUT_all_pred_12mtest_21mar.csv'\n",
    "    \n",
    "#     all_files_list=os.listdir(files_to_read_from)\n",
    "    all_data=pd.DataFrame()\n",
    "#     for i in range(len(all_files_list)):\n",
    "#         print(\"*******\",i,'*******')\n",
    "    filename=''\n",
    "    df=''\n",
    "    filename=all_files_list[i][:-4] #########\n",
    "    print('filename---',filename)\n",
    "#         data_=pd.DataFrame()\n",
    "#         df=files_to_read_from+'/'+all_files_list[i]\n",
    "#         print('df=',df)\n",
    "    data_=pd.read_csv(df)\n",
    "    print('data read')\n",
    "    data=pd.DataFrame(data_)\n",
    "        \n",
    "        #OUTBOUND\n",
    "    all_data=all_data.append(data[['CALL_YEAR_MONTH','SRC_TADIG','DST_TADIG','CALL_TYPE','SRC_COUNTRY','DST_COUNTRY']])\n",
    "        \n",
    "        #INBOUND\n",
    "#         all_data=all_data.append(data[['CALL_YEAR_MONTH','SRC_TADIG','DST_TADIG','CALL_TYPE','SRC_COUNTRY','DST_COUNTRY','DST_NETWORK']].head(1))\n",
    "        \n",
    "#         END_DATE= data['CALL_YEAR_MONTH'].iloc[-1]\n",
    "#         print(END_DATE)\n",
    "#         all_data['END_DATE']=END_DATE\n",
    "# #         all_data=all_data.append(data.tail(1))\n",
    "#         print('length =',len(all_data))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/MalikM/Documents/VRS -  Roaming Services/MOC_OUTBOUND/MOC_OUTBOUND_OUTPUT_36_12/MOC_OUT_all_pred_12mtest_21mar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=['months','file_name','PointForecast','ModelName','Mape','Param_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1=data[data['months'].isin([0,1,2,3,4,5,6,7,8,9,10,11])]\n",
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2=data_1.groupby(['file_name']).mean()\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_3=data_2.loc[: , [\"file_name\",\"PointForecast\",\"Mape\"]]\n",
    "# data_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
